{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":16295,"databundleVersionId":1099992,"sourceType":"competition"},{"sourceId":1074900,"sourceType":"datasetVersion","datasetId":597869},{"sourceId":31965447,"sourceType":"kernelVersion"}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-25T21:37:26.045920Z","iopub.execute_input":"2023-11-25T21:37:26.046273Z","iopub.status.idle":"2023-11-25T21:37:26.402867Z","shell.execute_reply.started":"2023-11-25T21:37:26.046247Z","shell.execute_reply":"2023-11-25T21:37:26.401999Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/input/tweet-sentiment-extraction/sample_submission.csv\n/kaggle/input/tweet-sentiment-extraction/train.csv\n/kaggle/input/tweet-sentiment-extraction/test.csv\n/kaggle/input/tf-roberta/pretrained-roberta-base.h5\n/kaggle/input/tf-roberta/config-roberta-base.json\n/kaggle/input/tf-roberta/vocab-roberta-base.json\n/kaggle/input/tf-roberta/merges-roberta-base.txt\n/kaggle/input/tensorflow-roberta-0-705/v0-roberta-2.h5\n/kaggle/input/tensorflow-roberta-0-705/v0-roberta-3.h5\n/kaggle/input/tensorflow-roberta-0-705/v0-roberta-1.h5\n/kaggle/input/tensorflow-roberta-0-705/v0-roberta-4.h5\n/kaggle/input/tensorflow-roberta-0-705/__results__.html\n/kaggle/input/tensorflow-roberta-0-705/submission.csv\n/kaggle/input/tensorflow-roberta-0-705/__resultx__.html\n/kaggle/input/tensorflow-roberta-0-705/__notebook__.ipynb\n/kaggle/input/tensorflow-roberta-0-705/v0-roberta-0.h5\n/kaggle/input/tensorflow-roberta-0-705/__output__.json\n/kaggle/input/tensorflow-roberta-0-705/custom.css\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd, numpy as np\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom sklearn.model_selection import StratifiedKFold\nfrom transformers import *\nimport tokenizers\nprint('TF version',tf.__version__)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-25T21:37:28.010018Z","iopub.execute_input":"2023-11-25T21:37:28.010801Z","iopub.status.idle":"2023-11-25T21:40:10.878995Z","shell.execute_reply.started":"2023-11-25T21:37:28.010772Z","shell.execute_reply":"2023-11-25T21:40:10.877990Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation_tf_utils.py:24: FutureWarning: Importing `TFGenerationMixin` from `src/transformers/generation_tf_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import TFGenerationMixin` instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation_flax_utils.py:24: FutureWarning: Importing `FlaxGenerationMixin` from `src/transformers/generation_flax_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import FlaxGenerationMixin` instead.\n  warnings.warn(\nLoading custom CUDA kernels...\n","output_type":"stream"},{"name":"stdout","text":"Using TensorFlow backend\n","output_type":"stream"},{"name":"stderr","text":"Loading custom CUDA kernels...\nUsing /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\nCreating extension directory /root/.cache/torch_extensions/py310_cu118/cuda_kernel...\nDetected CUDA files, patching ldflags\nEmitting ninja build file /root/.cache/torch_extensions/py310_cu118/cuda_kernel/build.ninja...\nBuilding extension module cuda_kernel...\nAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n","output_type":"stream"},{"name":"stdout","text":"[1/4] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=cuda_kernel -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1016\\\" -isystem /opt/conda/lib/python3.10/site-packages/torch/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 --compiler-options '-fPIC' -std=c++17 -c /opt/conda/lib/python3.10/site-packages/transformers/kernels/mra/cuda_kernel.cu -o cuda_kernel.cuda.o \n[2/4] c++ -MMD -MF torch_extension.o.d -DTORCH_EXTENSION_NAME=cuda_kernel -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1016\\\" -isystem /opt/conda/lib/python3.10/site-packages/torch/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=1 -fPIC -std=c++17 -c /opt/conda/lib/python3.10/site-packages/transformers/kernels/mra/torch_extension.cpp -o torch_extension.o \n[3/4] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=cuda_kernel -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1016\\\" -isystem /opt/conda/lib/python3.10/site-packages/torch/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_60,code=compute_60 -gencode=arch=compute_60,code=sm_60 --compiler-options '-fPIC' -std=c++17 -c /opt/conda/lib/python3.10/site-packages/transformers/kernels/mra/cuda_launch.cu -o cuda_launch.cuda.o \n[4/4] c++ cuda_kernel.cuda.o cuda_launch.cuda.o torch_extension.o -shared -L/opt/conda/lib/python3.10/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o cuda_kernel.so\n","output_type":"stream"},{"name":"stderr","text":"Loading extension module cuda_kernel...\n","output_type":"stream"},{"name":"stdout","text":"TF version 2.13.0\n","output_type":"stream"}]},{"cell_type":"code","source":"import tokenizers\n\n# Print the version of the tokenizers library\nprint(tokenizers.__version__)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-25T21:40:10.880712Z","iopub.execute_input":"2023-11-25T21:40:10.881419Z","iopub.status.idle":"2023-11-25T21:40:10.886684Z","shell.execute_reply.started":"2023-11-25T21:40:10.881393Z","shell.execute_reply":"2023-11-25T21:40:10.885775Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"0.14.1\n","output_type":"stream"}]},{"cell_type":"code","source":"import tokenizers\n\nMAX_LEN = 96\nPATH = '/kaggle/input/tf-roberta'\n\n# Load the tokenizer using the from_file method\ntokenizer = tokenizers.ByteLevelBPETokenizer.from_file(\n    vocab_filename='/kaggle/input/tf-roberta/vocab-roberta-base.json',\n    merges_filename='/kaggle/input/tf-roberta/merges-roberta-base.txt',\n\n    lowercase=True,\n    add_prefix_space=True\n)\n\nsentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}\ntrain = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv').fillna('')\n","metadata":{"execution":{"iopub.status.busy":"2023-11-25T21:40:10.887713Z","iopub.execute_input":"2023-11-25T21:40:10.888039Z","iopub.status.idle":"2023-11-25T21:40:11.290366Z","shell.execute_reply.started":"2023-11-25T21:40:10.888015Z","shell.execute_reply":"2023-11-25T21:40:11.289376Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import os\n\n# Print the content of the directory\nprint(os.listdir('/kaggle/input/tf-roberta/'))\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-25T21:40:11.292639Z","iopub.execute_input":"2023-11-25T21:40:11.292961Z","iopub.status.idle":"2023-11-25T21:40:11.298131Z","shell.execute_reply.started":"2023-11-25T21:40:11.292929Z","shell.execute_reply":"2023-11-25T21:40:11.297226Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"['pretrained-roberta-base.h5', 'config-roberta-base.json', 'vocab-roberta-base.json', 'merges-roberta-base.txt']\n","output_type":"stream"}]},{"cell_type":"code","source":"ct = train.shape[0]\ninput_ids = np.ones((ct,MAX_LEN),dtype='int32')\nattention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\ntoken_type_ids = np.zeros((ct,MAX_LEN),dtype='int32')\nstart_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\nend_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n\nfor k in range(train.shape[0]):\n    \n    # FIND OVERLAP\n    text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n    text2 = \" \".join(train.loc[k,'selected_text'].split())\n    idx = text1.find(text2)\n    chars = np.zeros((len(text1)))\n    chars[idx:idx+len(text2)]=1\n    if text1[idx-1]==' ': chars[idx-1] = 1 \n    enc = tokenizer.encode(text1) \n        \n    # ID_OFFSETS\n    offsets = []; idx=0\n    for t in enc.ids:\n        w = tokenizer.decode([t])\n        offsets.append((idx,idx+len(w)))\n        idx += len(w)\n    \n    # START END TOKENS\n    toks = []\n    for i,(a,b) in enumerate(offsets):\n        sm = np.sum(chars[a:b])\n        if sm>0: toks.append(i) \n        \n    s_tok = sentiment_id[train.loc[k,'sentiment']]\n    input_ids[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n    attention_mask[k,:len(enc.ids)+5] = 1\n    if len(toks)>0:\n        start_tokens[k,toks[0]+1] = 1\n        end_tokens[k,toks[-1]+1] = 1","metadata":{"execution":{"iopub.status.busy":"2023-11-25T21:40:11.299236Z","iopub.execute_input":"2023-11-25T21:40:11.299572Z","iopub.status.idle":"2023-11-25T21:40:19.735216Z","shell.execute_reply.started":"2023-11-25T21:40:11.299547Z","shell.execute_reply":"2023-11-25T21:40:19.734467Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv('../input/tweet-sentiment-extraction/test.csv').fillna('')\n\nct = test.shape[0]\ninput_ids_t = np.ones((ct,MAX_LEN),dtype='int32')\nattention_mask_t = np.zeros((ct,MAX_LEN),dtype='int32')\ntoken_type_ids_t = np.zeros((ct,MAX_LEN),dtype='int32')\n\nfor k in range(test.shape[0]):\n        \n    # INPUT_IDS\n    text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n    enc = tokenizer.encode(text1)                \n    s_tok = sentiment_id[test.loc[k,'sentiment']]\n    input_ids_t[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n    attention_mask_t[k,:len(enc.ids)+5] = 1","metadata":{"execution":{"iopub.status.busy":"2023-11-25T21:40:19.736319Z","iopub.execute_input":"2023-11-25T21:40:19.736627Z","iopub.status.idle":"2023-11-25T21:40:20.124712Z","shell.execute_reply.started":"2023-11-25T21:40:19.736602Z","shell.execute_reply":"2023-11-25T21:40:20.123726Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# from tensorflow.keras import layers as L\n# from tensorflow.keras.models import Model\n# import tensorflow as tf\n# from tensorflow.keras.optimizers import Adam\n\n# from tensorflow.keras.layers import MultiHeadAttention\n\n\n# #multi-head self-attention model \n\n# def build_model():\n#     ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n#     att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n#     tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n\n#     # Load RoBERTa model\n#     config = RobertaConfig.from_json_file('/kaggle/input/tf-roberta/config-roberta-base.json')\n#     bert_model = TFRobertaModel.from_pretrained('/kaggle/input/tf-roberta/pretrained-roberta-base.h5', config=config)\n#     x = bert_model(ids, attention_mask=att, token_type_ids=tok)\n\n#     # Add multi-head self-attention\n#     key_dim = 128  # You can adjust the key_dim\n#     attention_heads = 8  # You can adjust the number of attention heads\n#     x_att = MultiHeadAttention(num_heads=attention_heads, key_dim=key_dim)(x[0], x[0], x[0])  # Use MultiHeadAttention\n#     x = tf.keras.layers.Add()([x[0], x_att])\n\n#     # Continue with your model\n#     x1 = tf.keras.layers.Dropout(0.1)(x)\n#     x1 = tf.keras.layers.Conv1D(1, 1)(x1)\n#     x1 = tf.keras.layers.Flatten()(x1)\n#     x1 = tf.keras.layers.Activation('softmax')(x1)\n\n#     x2 = tf.keras.layers.Dropout(0.1)(x)\n#     x2 = tf.keras.layers.Conv1D(1, 1)(x2)\n#     x2 = tf.keras.layers.Flatten()(x2)\n#     x2 = tf.keras.layers.Activation('softmax')(x2)\n\n#     model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1, x2])\n#     optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n#     model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n\n#     return model\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-25T23:01:48.929961Z","iopub.execute_input":"2023-11-25T23:01:48.930663Z","iopub.status.idle":"2023-11-25T23:01:48.942146Z","shell.execute_reply.started":"2023-11-25T23:01:48.930631Z","shell.execute_reply":"2023-11-25T23:01:48.941147Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"\n# #original- baseline model\n\n# def build_model():\n#     ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n#     att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n#     tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n#     config = RobertaConfig.from_json_file('/kaggle/input/tf-roberta/config-roberta-base.json')\n#     #config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n#     #bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n#     bert_model = TFRobertaModel.from_pretrained('/kaggle/input/tf-roberta/pretrained-roberta-base.h5',config=config)\n#     x = bert_model(ids,attention_mask=att,token_type_ids=tok)\n    \n#     x1 = tf.keras.layers.Dropout(0.1)(x[0]) \n#     x1 = tf.keras.layers.Conv1D(1,1)(x1)\n#     x1 = tf.keras.layers.Flatten()(x1)\n#     x1 = tf.keras.layers.Activation('softmax')(x1)\n    \n#     x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n#     x2 = tf.keras.layers.Conv1D(1,1)(x2)\n#     x2 = tf.keras.layers.Flatten()(x2)\n#     x2 = tf.keras.layers.Activation('softmax')(x2)\n\n#     model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n#     optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n#     model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n\n#     return model","metadata":{"execution":{"iopub.status.busy":"2023-11-25T21:41:35.283076Z","iopub.execute_input":"2023-11-25T21:41:35.284213Z","iopub.status.idle":"2023-11-25T21:41:35.293722Z","shell.execute_reply.started":"2023-11-25T21:41:35.284171Z","shell.execute_reply":"2023-11-25T21:41:35.292783Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# #dynamic attention model \n# import tensorflow as tf\n# from tensorflow.keras.layers import Layer\n\n# class DynamicAttention(Layer):\n#     def __init__(self, key_dim, value_dim, **kwargs):\n#         super(DynamicAttention, self).__init__(**kwargs)\n#         self.key_dim = key_dim\n#         self.value_dim = value_dim\n#         self.dense_query = tf.keras.layers.Dense(self.key_dim, activation='relu')\n#         self.dense_key = tf.keras.layers.Dense(self.key_dim, activation='relu')\n#         self.linear_projection_query = tf.keras.layers.Dense(self.value_dim)\n#         self.linear_projection_key = tf.keras.layers.Dense(self.value_dim)\n#         self.attention = tf.keras.layers.Attention(use_scale=True)\n\n#     def call(self, inputs):\n#         query, key, value = inputs\n\n#         # Ensure the dimensions match\n#         processed_query = self.dense_query(query)\n#         processed_key = self.dense_key(key)\n\n#         # Add linear projections to match the attention mechanism dimensions\n#         linear_projection_query = self.linear_projection_query(processed_query)\n#         linear_projection_key = self.linear_projection_key(processed_key)\n\n#         context = self.attention([linear_projection_query, linear_projection_key, value])\n\n#         return context\n\n\n# from tensorflow.keras.layers import Input, Dropout, Conv1D, Flatten, Activation\n# from transformers import RobertaConfig, TFRobertaModel\n\n# #dynamic attention model \n\n# def build_model():\n#     ids = Input((MAX_LEN,), dtype=tf.int32)\n#     att = Input((MAX_LEN,), dtype=tf.int32)\n#     tok = Input((MAX_LEN,), dtype=tf.int32)\n\n#     config = RobertaConfig.from_json_file('/kaggle/input/tf-roberta/config-roberta-base.json')\n#     bert_model = TFRobertaModel.from_pretrained('/kaggle/input/tf-roberta/pretrained-roberta-base.h5', config=config)\n    \n#     x = bert_model(ids, attention_mask=att, token_type_ids=tok)\n\n#     key_dim = 768\n#     value_dim = 768  # You can adjust this value\n#     dynamic_attention = DynamicAttention(key_dim=key_dim, value_dim=value_dim)([x[0], x[0], x[0]])  # Dynamic attention\n\n\n#     x_att = tf.keras.layers.Add()([x[0], dynamic_attention])\n\n#     x1 = Dropout(0.1)(x_att)\n#     x1 = Conv1D(1, 1)(x1)\n#     x1 = Flatten()(x1)\n#     x1 = Activation('softmax')(x1)\n\n#     x2 = Dropout(0.1)(x_att)\n#     x2 = Conv1D(1, 1)(x2)\n#     x2 = Flatten()(x2)\n#     x2 = Activation('softmax')(x2)\n\n#     model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1, x2])\n#     optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n#     model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n\n#     return model","metadata":{"execution":{"iopub.status.busy":"2023-11-26T00:21:54.486037Z","iopub.execute_input":"2023-11-26T00:21:54.486924Z","iopub.status.idle":"2023-11-26T00:21:54.502159Z","shell.execute_reply.started":"2023-11-26T00:21:54.486893Z","shell.execute_reply":"2023-11-26T00:21:54.501238Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"\n# import tensorflow as tf\n# from tensorflow.keras.layers import Layer\n\n\n# #muti-head dynamic attention model\n# class MultiHeadDynamicAttention(Layer):\n#     def __init__(self, key_dim, value_dim, num_heads, **kwargs):\n#         super(MultiHeadDynamicAttention, self).__init__(**kwargs)\n#         self.key_dim = key_dim\n#         self.value_dim = value_dim\n#         self.num_heads = num_heads\n\n#         self.dense_query = [tf.keras.layers.Dense(self.key_dim, activation='relu') for _ in range(num_heads)]\n#         self.dense_key = [tf.keras.layers.Dense(self.key_dim, activation='relu') for _ in range(num_heads)]\n#         self.linear_projection_query = [tf.keras.layers.Dense(self.value_dim) for _ in range(num_heads)]\n#         self.linear_projection_key = [tf.keras.layers.Dense(self.value_dim) for _ in range(num_heads)]\n#         self.attention = tf.keras.layers.Attention(use_scale=True)\n\n#     def call(self, inputs):\n#         query, key, value = inputs\n\n#         # Ensure the dimensions match\n#         processed_queries = [dense(query) for dense in self.dense_query]\n#         processed_keys = [dense(key) for dense in self.dense_key]\n\n#         # Add linear projections to match the attention mechanism dimensions\n# #         linear_projections_query = [linear(processed_query) for linear, processed_query in zip(self.linear_projection_query, processed_queries)]\n# #         linear_projections_key = [linear(processed_key) for linear, processed_key in zip(self.linear_projection_key, processed_keys)]\n#         linear_projections_query = [dense_linear(processed_query) for dense_linear, processed_query in zip(self.linear_projection_query, processed_queries)]\n#         linear_projections_key = [dense_linear(processed_key) for dense_linear, processed_key in zip(self.linear_projection_key, processed_keys)]\n\n\n#         # Perform attention for each head\n#         attentions = [self.attention([linear_projection_query, linear_projection_key, value]) for linear_projection_query, linear_projection_key in zip(linear_projections_query, linear_projections_key)]\n\n#         # Concatenate or average the outputs from different heads\n#         concatenated_attentions = tf.concat(attentions, axis=-1)\n#         averaged_attention = tf.reduce_mean(attentions, axis=0)\n\n#         return averaged_attention  # You can return concatenated_attentions if you prefer concatenation\n\n    \n\n\n# #muti-head dynamic attention model \n\n# from tensorflow.keras.layers import Input, Dropout, Conv1D, Flatten, Activation\n# from transformers import RobertaConfig, TFRobertaModel\n# from tensorflow.keras.optimizers.schedules import ExponentialDecay\n\n# def build_model():\n#     ids = Input((MAX_LEN,), dtype=tf.int32)\n#     att = Input((MAX_LEN,), dtype=tf.int32)\n#     tok = Input((MAX_LEN,), dtype=tf.int32)\n\n#     config = RobertaConfig.from_json_file('/kaggle/input/tf-roberta/config-roberta-base.json')\n#     bert_model = TFRobertaModel.from_pretrained('/kaggle/input/tf-roberta/pretrained-roberta-base.h5', config=config)\n    \n#     x = bert_model(ids, attention_mask=att, token_type_ids=tok)\n\n#     key_dim = 768\n#     value_dim = 768  # You can adjust this value\n#     num_heads = 4  # You can adjust the number of heads\n#     dynamic_attention = MultiHeadDynamicAttention(key_dim=key_dim, value_dim=value_dim, num_heads=num_heads)([x[0], x[0], x[0]])\n\n\n\n#     x_att = tf.keras.layers.Add()([x[0], dynamic_attention])\n\n#     x1 = Dropout(0.1)(x_att)\n#     x1 = Conv1D(1, 1)(x1)\n#     x1 = Flatten()(x1)\n#     x1 = Activation('softmax')(x1)\n\n#     x2 = Dropout(0.1)(x_att)\n#     x2 = Conv1D(1, 1)(x2)\n#     x2 = Flatten()(x2)\n#     x2 = Activation('softmax')(x2)\n\n#     model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1, x2])\n#     optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n#     model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n\n#     return model","metadata":{"execution":{"iopub.status.busy":"2023-11-26T03:27:28.774151Z","iopub.execute_input":"2023-11-26T03:27:28.774518Z","iopub.status.idle":"2023-11-26T03:27:28.794072Z","shell.execute_reply.started":"2023-11-26T03:27:28.774477Z","shell.execute_reply":"2023-11-26T03:27:28.793014Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Layer\n\n\n#muti-head dynamic attention model\nclass MultiHeadDynamicAttention(Layer):\n    def __init__(self, key_dim, value_dim, num_heads, **kwargs):\n        super(MultiHeadDynamicAttention, self).__init__(**kwargs)\n        self.key_dim = key_dim\n        self.value_dim = value_dim\n        self.num_heads = num_heads\n\n        self.dense_query = [tf.keras.layers.Dense(self.key_dim, activation='relu') for _ in range(num_heads)]\n        self.dense_key = [tf.keras.layers.Dense(self.key_dim, activation='relu') for _ in range(num_heads)]\n        self.linear_projection_query = [tf.keras.layers.Dense(self.value_dim) for _ in range(num_heads)]\n        self.linear_projection_key = [tf.keras.layers.Dense(self.value_dim) for _ in range(num_heads)]\n        self.attention = tf.keras.layers.Attention(use_scale=True)\n\n    def call(self, inputs):\n        query, key, value = inputs\n\n        # Ensure the dimensions match\n        processed_queries = [dense(query) for dense in self.dense_query]\n        processed_keys = [dense(key) for dense in self.dense_key]\n\n        # Add linear projections to match the attention mechanism dimensions\n#         linear_projections_query = [linear(processed_query) for linear, processed_query in zip(self.linear_projection_query, processed_queries)]\n#         linear_projections_key = [linear(processed_key) for linear, processed_key in zip(self.linear_projection_key, processed_keys)]\n        linear_projections_query = [dense_linear(processed_query) for dense_linear, processed_query in zip(self.linear_projection_query, processed_queries)]\n        linear_projections_key = [dense_linear(processed_key) for dense_linear, processed_key in zip(self.linear_projection_key, processed_keys)]\n\n\n        # Perform attention for each head\n        attentions = [self.attention([linear_projection_query, linear_projection_key, value]) for linear_projection_query, linear_projection_key in zip(linear_projections_query, linear_projections_key)]\n\n        # Concatenate or average the outputs from different heads\n        concatenated_attentions = tf.concat(attentions, axis=-1)\n        averaged_attention = tf.reduce_mean(attentions, axis=0)\n\n        return concatenated_attentions  # You can return concatenated_attentions if you prefer concatenation\n\n    \n\n\n#muti-head dynamic attention model \n\nfrom tensorflow.keras.layers import Input, Dropout, Conv1D, Flatten, Activation\nfrom transformers import RobertaConfig, TFRobertaModel\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\n\ndef build_model():\n    ids = Input((MAX_LEN,), dtype=tf.int32)\n    att = Input((MAX_LEN,), dtype=tf.int32)\n    tok = Input((MAX_LEN,), dtype=tf.int32)\n\n    config = RobertaConfig.from_json_file('/kaggle/input/tf-roberta/config-roberta-base.json')\n    bert_model = TFRobertaModel.from_pretrained('/kaggle/input/tf-roberta/pretrained-roberta-base.h5', config=config)\n    \n    x = bert_model(ids, attention_mask=att, token_type_ids=tok)\n\n    key_dim = 768\n    value_dim = 768  # You can adjust this value\n    num_heads = 4  # You can adjust the number of heads\n    dynamic_attention = MultiHeadDynamicAttention(key_dim=key_dim, value_dim=value_dim, num_heads=num_heads)([x[0], x[0], x[0]])\n\n\n\n    #x_att = tf.keras.layers.Add()([x[0], dynamic_attention])\n    x_att = tf.keras.layers.Concatenate(axis=-1)([x[0], dynamic_attention])\n\n\n    x1 = Dropout(0.1)(x_att)\n    x1 = Conv1D(1, 1)(x1)\n    x1 = Flatten()(x1)\n    x1 = Activation('softmax')(x1)\n\n    x2 = Dropout(0.1)(x_att)\n    x2 = Conv1D(1, 1)(x2)\n    x2 = Flatten()(x2)\n    x2 = Activation('softmax')(x2)\n\n    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1, x2])\n    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-11-26T03:34:01.068752Z","iopub.execute_input":"2023-11-26T03:34:01.069539Z","iopub.status.idle":"2023-11-26T03:34:01.089565Z","shell.execute_reply.started":"2023-11-26T03:34:01.069508Z","shell.execute_reply":"2023-11-26T03:34:01.088505Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    if (len(a)==0) & (len(b)==0): return 0.5\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))","metadata":{"execution":{"iopub.status.busy":"2023-11-26T03:34:02.253038Z","iopub.execute_input":"2023-11-26T03:34:02.253870Z","iopub.status.idle":"2023-11-26T03:34:02.259391Z","shell.execute_reply.started":"2023-11-26T03:34:02.253828Z","shell.execute_reply":"2023-11-26T03:34:02.258465Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"# jac = []; VER='v0'; DISPLAY=1 # USE display=1 FOR INTERACTIVE\n# oof_start = np.zeros((input_ids.shape[0], MAX_LEN))\n# oof_end = np.zeros((input_ids.shape[0], MAX_LEN))\n# preds_start = np.zeros((input_ids_t.shape[0], MAX_LEN))\n# preds_end = np.zeros((input_ids_t.shape[0], MAX_LEN))\n\n# fold = 0  # Specify the fold you want to train\n\n# print('#' * 25)\n# print('### FOLD %i' % (fold + 1))\n# print('#' * 25)\n\n# K.clear_session()\n# model = build_model()\n\n# sv = tf.keras.callbacks.ModelCheckpoint(\n#     '%s-roberta-%i.h5' % (VER, fold), monitor='val_loss', verbose=1, save_best_only=True,\n#     save_weights_only=True, mode='auto', save_freq='epoch')\n\n# # Use StratifiedKFold to get a single split for the specified fold\n# idxT, idxV = next(iter(StratifiedKFold(n_splits=5, shuffle=True, random_state=777).split(input_ids, train.sentiment.values)))\n\n# model.fit(\n#     [input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]],\n#     [start_tokens[idxT,], end_tokens[idxT,]],\n#     epochs=1, batch_size=32, verbose=DISPLAY, callbacks=[sv],\n#     validation_data=(\n#         [input_ids[idxV,], attention_mask[idxV,], token_type_ids[idxV,]],\n#         [start_tokens[idxV,], end_tokens[idxV,]]\n#     )\n# )\n\n\n\n\n# print('Loading model...')\n# model.load_weights('%s-roberta-%i.h5' % (VER, fold))\n\n# print('Predicting OOF...')\n# oof_start[idxV,], oof_end[idxV,] = model.predict(\n#     [input_ids[idxV,], attention_mask[idxV,], token_type_ids[idxV,]],\n#     verbose=DISPLAY\n# )\n\n# print('Predicting Test...')\n# preds = model.predict([input_ids_t, attention_mask_t, token_type_ids_t], verbose=DISPLAY)\n# preds_start += preds[0]\n# preds_end += preds[1]\n\n# # Display Jaccard for the single fold\n# all = []\n# for k in idxV:\n#     a = np.argmax(oof_start[k,])\n#     b = np.argmax(oof_end[k,])\n#     if a > b:\n#         st = train.loc[k, 'text']\n#     else:\n#         text1 = \" \" + \" \".join(train.loc[k, 'text'].split())\n#         enc = tokenizer.encode(text1)\n#         st = tokenizer.decode(enc.ids[a - 1:b])\n#     all.append(jaccard(st, train.loc[k, 'selected_text']))\n\n# print('>>>> FOLD %i Jaccard =' % (fold + 1), np.mean(all))\n","metadata":{"execution":{"iopub.status.busy":"2023-11-26T03:34:04.351327Z","iopub.execute_input":"2023-11-26T03:34:04.352183Z","iopub.status.idle":"2023-11-26T03:34:04.358081Z","shell.execute_reply.started":"2023-11-26T03:34:04.352150Z","shell.execute_reply":"2023-11-26T03:34:04.357144Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"#5 fold 3 epochs trainning\njac = []; VER='v0'; DISPLAY=1 # USE display=1 FOR INTERACTIVE\noof_start = np.zeros((input_ids.shape[0],MAX_LEN))\noof_end = np.zeros((input_ids.shape[0],MAX_LEN))\npreds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\npreds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\n\nskf = StratifiedKFold(n_splits=5,shuffle=True,random_state=777)\nfor fold,(idxT,idxV) in enumerate(skf.split(input_ids,train.sentiment.values)):\n\n    print('#'*25)\n    print('### FOLD %i'%(fold+1))\n    print('#'*25)\n    \n    K.clear_session()\n    model = build_model()\n        \n    sv = tf.keras.callbacks.ModelCheckpoint(\n        '%s-roberta-%i.h5'%(VER,fold), monitor='val_loss', verbose=1, save_best_only=True,\n        save_weights_only=True, mode='auto', save_freq='epoch')\n        \n    model.fit([input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]], [start_tokens[idxT,], end_tokens[idxT,]], \n        epochs=3, batch_size=32, verbose=DISPLAY, callbacks=[sv],\n        validation_data=([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]], \n        [start_tokens[idxV,], end_tokens[idxV,]]))\n    \n    print('Loading model...')\n    model.load_weights('%s-roberta-%i.h5'%(VER,fold))\n    \n    print('Predicting OOF...')\n    oof_start[idxV,],oof_end[idxV,] = model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose=DISPLAY)\n    \n    print('Predicting Test...')\n    preds = model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n    preds_start += preds[0]/skf.n_splits\n    preds_end += preds[1]/skf.n_splits\n    \n    # DISPLAY FOLD JACCARD\n    all = []\n    for k in idxV:\n        a = np.argmax(oof_start[k,])\n        b = np.argmax(oof_end[k,])\n        if a>b: \n            st = train.loc[k,'text'] # IMPROVE CV/LB with better choice here\n        else:\n            text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n            enc = tokenizer.encode(text1)\n            st = tokenizer.decode(enc.ids[a-1:b])\n        all.append(jaccard(st,train.loc[k,'selected_text']))\n    jac.append(np.mean(all))\n    print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(all))\n    print()","metadata":{"execution":{"iopub.status.busy":"2023-11-26T03:34:04.957790Z","iopub.execute_input":"2023-11-26T03:34:04.958150Z","iopub.status.idle":"2023-11-26T04:59:13.072717Z","shell.execute_reply.started":"2023-11-26T03:34:04.958123Z","shell.execute_reply":"2023-11-26T04:59:13.071631Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stderr","text":"loading weights file /kaggle/input/tf-roberta/pretrained-roberta-base.h5\n","output_type":"stream"},{"name":"stdout","text":"#########################\n### FOLD 1\n#########################\n","output_type":"stream"},{"name":"stderr","text":"All model checkpoint layers were used when initializing TFRobertaModel.\n\nAll the layers of TFRobertaModel were initialized from the model checkpoint at /kaggle/input/tf-roberta/pretrained-roberta-base.h5.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3\n687/687 [==============================] - ETA: 0s - loss: 2.3619 - activation_loss: 1.1604 - activation_1_loss: 1.2016\nEpoch 1: val_loss improved from inf to 1.74552, saving model to v0-roberta-0.h5\n687/687 [==============================] - 343s 431ms/step - loss: 2.3619 - activation_loss: 1.1604 - activation_1_loss: 1.2016 - val_loss: 1.7455 - val_activation_loss: 0.8913 - val_activation_1_loss: 0.8542\nEpoch 2/3\n687/687 [==============================] - ETA: 0s - loss: 1.7186 - activation_loss: 0.8719 - activation_1_loss: 0.8468\nEpoch 2: val_loss improved from 1.74552 to 1.72995, saving model to v0-roberta-0.h5\n687/687 [==============================] - 290s 422ms/step - loss: 1.7186 - activation_loss: 0.8719 - activation_1_loss: 0.8468 - val_loss: 1.7300 - val_activation_loss: 0.8827 - val_activation_1_loss: 0.8473\nEpoch 3/3\n687/687 [==============================] - ETA: 0s - loss: 1.5804 - activation_loss: 0.8169 - activation_1_loss: 0.7635\nEpoch 3: val_loss improved from 1.72995 to 1.71482, saving model to v0-roberta-0.h5\n687/687 [==============================] - 290s 422ms/step - loss: 1.5804 - activation_loss: 0.8169 - activation_1_loss: 0.7635 - val_loss: 1.7148 - val_activation_loss: 0.8702 - val_activation_1_loss: 0.8446\nLoading model...\nPredicting OOF...\n172/172 [==============================] - 28s 143ms/step\nPredicting Test...\n111/111 [==============================] - 16s 143ms/step\n","output_type":"stream"},{"name":"stderr","text":"loading weights file /kaggle/input/tf-roberta/pretrained-roberta-base.h5\n","output_type":"stream"},{"name":"stdout","text":">>>> FOLD 1 Jaccard = 0.6991355787953382\n\n#########################\n### FOLD 2\n#########################\n","output_type":"stream"},{"name":"stderr","text":"All model checkpoint layers were used when initializing TFRobertaModel.\n\nAll the layers of TFRobertaModel were initialized from the model checkpoint at /kaggle/input/tf-roberta/pretrained-roberta-base.h5.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3\n688/688 [==============================] - ETA: 0s - loss: 2.4655 - activation_loss: 1.2184 - activation_1_loss: 1.2471\nEpoch 1: val_loss improved from inf to 1.73996, saving model to v0-roberta-1.h5\n688/688 [==============================] - 361s 448ms/step - loss: 2.4655 - activation_loss: 1.2184 - activation_1_loss: 1.2471 - val_loss: 1.7400 - val_activation_loss: 0.9043 - val_activation_1_loss: 0.8356\nEpoch 2/3\n688/688 [==============================] - ETA: 0s - loss: 1.7376 - activation_loss: 0.8926 - activation_1_loss: 0.8451\nEpoch 2: val_loss improved from 1.73996 to 1.66815, saving model to v0-roberta-1.h5\n688/688 [==============================] - 303s 440ms/step - loss: 1.7376 - activation_loss: 0.8926 - activation_1_loss: 0.8451 - val_loss: 1.6682 - val_activation_loss: 0.8456 - val_activation_1_loss: 0.8226\nEpoch 3/3\n688/688 [==============================] - ETA: 0s - loss: 1.6294 - activation_loss: 0.8382 - activation_1_loss: 0.7912\nEpoch 3: val_loss improved from 1.66815 to 1.64041, saving model to v0-roberta-1.h5\n688/688 [==============================] - 303s 440ms/step - loss: 1.6294 - activation_loss: 0.8382 - activation_1_loss: 0.7912 - val_loss: 1.6404 - val_activation_loss: 0.8421 - val_activation_1_loss: 0.7983\nLoading model...\nPredicting OOF...\n172/172 [==============================] - 28s 142ms/step\nPredicting Test...\n111/111 [==============================] - 16s 142ms/step\n","output_type":"stream"},{"name":"stderr","text":"loading weights file /kaggle/input/tf-roberta/pretrained-roberta-base.h5\n","output_type":"stream"},{"name":"stdout","text":">>>> FOLD 2 Jaccard = 0.705683486004461\n\n#########################\n### FOLD 3\n#########################\n","output_type":"stream"},{"name":"stderr","text":"All model checkpoint layers were used when initializing TFRobertaModel.\n\nAll the layers of TFRobertaModel were initialized from the model checkpoint at /kaggle/input/tf-roberta/pretrained-roberta-base.h5.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3\n688/688 [==============================] - ETA: 0s - loss: 2.3779 - activation_loss: 1.1538 - activation_1_loss: 1.2241\nEpoch 1: val_loss improved from inf to 1.81307, saving model to v0-roberta-2.h5\n688/688 [==============================] - 362s 449ms/step - loss: 2.3779 - activation_loss: 1.1538 - activation_1_loss: 1.2241 - val_loss: 1.8131 - val_activation_loss: 0.9090 - val_activation_1_loss: 0.9040\nEpoch 2/3\n688/688 [==============================] - ETA: 0s - loss: 1.7442 - activation_loss: 0.8825 - activation_1_loss: 0.8617\nEpoch 2: val_loss improved from 1.81307 to 1.67653, saving model to v0-roberta-2.h5\n688/688 [==============================] - 303s 440ms/step - loss: 1.7442 - activation_loss: 0.8825 - activation_1_loss: 0.8617 - val_loss: 1.6765 - val_activation_loss: 0.8635 - val_activation_1_loss: 0.8130\nEpoch 3/3\n688/688 [==============================] - ETA: 0s - loss: 1.6604 - activation_loss: 0.8519 - activation_1_loss: 0.8085\nEpoch 3: val_loss did not improve from 1.67653\n688/688 [==============================] - 302s 439ms/step - loss: 1.6604 - activation_loss: 0.8519 - activation_1_loss: 0.8085 - val_loss: 1.6873 - val_activation_loss: 0.8708 - val_activation_1_loss: 0.8164\nLoading model...\nPredicting OOF...\n172/172 [==============================] - 28s 143ms/step\nPredicting Test...\n111/111 [==============================] - 16s 143ms/step\n","output_type":"stream"},{"name":"stderr","text":"loading weights file /kaggle/input/tf-roberta/pretrained-roberta-base.h5\n","output_type":"stream"},{"name":"stdout","text":">>>> FOLD 3 Jaccard = 0.6988876266915874\n\n#########################\n### FOLD 4\n#########################\n","output_type":"stream"},{"name":"stderr","text":"All model checkpoint layers were used when initializing TFRobertaModel.\n\nAll the layers of TFRobertaModel were initialized from the model checkpoint at /kaggle/input/tf-roberta/pretrained-roberta-base.h5.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3\n688/688 [==============================] - ETA: 0s - loss: 2.3624 - activation_loss: 1.1629 - activation_1_loss: 1.1995\nEpoch 1: val_loss improved from inf to 1.78092, saving model to v0-roberta-3.h5\n688/688 [==============================] - 363s 450ms/step - loss: 2.3624 - activation_loss: 1.1629 - activation_1_loss: 1.1995 - val_loss: 1.7809 - val_activation_loss: 0.8851 - val_activation_1_loss: 0.8958\nEpoch 2/3\n688/688 [==============================] - ETA: 0s - loss: 1.6798 - activation_loss: 0.8580 - activation_1_loss: 0.8218\nEpoch 2: val_loss improved from 1.78092 to 1.62370, saving model to v0-roberta-3.h5\n688/688 [==============================] - 303s 440ms/step - loss: 1.6798 - activation_loss: 0.8580 - activation_1_loss: 0.8218 - val_loss: 1.6237 - val_activation_loss: 0.8249 - val_activation_1_loss: 0.7988\nEpoch 3/3\n688/688 [==============================] - ETA: 0s - loss: 1.5250 - activation_loss: 0.7895 - activation_1_loss: 0.7355\nEpoch 3: val_loss did not improve from 1.62370\n688/688 [==============================] - 301s 438ms/step - loss: 1.5250 - activation_loss: 0.7895 - activation_1_loss: 0.7355 - val_loss: 1.6263 - val_activation_loss: 0.8289 - val_activation_1_loss: 0.7973\nLoading model...\nPredicting OOF...\n172/172 [==============================] - 28s 143ms/step\nPredicting Test...\n111/111 [==============================] - 16s 143ms/step\n","output_type":"stream"},{"name":"stderr","text":"loading weights file /kaggle/input/tf-roberta/pretrained-roberta-base.h5\n","output_type":"stream"},{"name":"stdout","text":">>>> FOLD 4 Jaccard = 0.7053618196174949\n\n#########################\n### FOLD 5\n#########################\n","output_type":"stream"},{"name":"stderr","text":"All model checkpoint layers were used when initializing TFRobertaModel.\n\nAll the layers of TFRobertaModel were initialized from the model checkpoint at /kaggle/input/tf-roberta/pretrained-roberta-base.h5.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3\n688/688 [==============================] - ETA: 0s - loss: 2.4660 - activation_loss: 1.1895 - activation_1_loss: 1.2765\nEpoch 1: val_loss improved from inf to 1.83832, saving model to v0-roberta-4.h5\n688/688 [==============================] - 362s 449ms/step - loss: 2.4660 - activation_loss: 1.1895 - activation_1_loss: 1.2765 - val_loss: 1.8383 - val_activation_loss: 0.8819 - val_activation_1_loss: 0.9564\nEpoch 2/3\n688/688 [==============================] - ETA: 0s - loss: 1.9521 - activation_loss: 0.9092 - activation_1_loss: 1.0429\nEpoch 2: val_loss improved from 1.83832 to 1.67398, saving model to v0-roberta-4.h5\n688/688 [==============================] - 303s 440ms/step - loss: 1.9521 - activation_loss: 0.9092 - activation_1_loss: 1.0429 - val_loss: 1.6740 - val_activation_loss: 0.8324 - val_activation_1_loss: 0.8415\nEpoch 3/3\n688/688 [==============================] - ETA: 0s - loss: 1.6489 - activation_loss: 0.8294 - activation_1_loss: 0.8194\nEpoch 3: val_loss improved from 1.67398 to 1.66706, saving model to v0-roberta-4.h5\n688/688 [==============================] - 303s 440ms/step - loss: 1.6489 - activation_loss: 0.8294 - activation_1_loss: 0.8194 - val_loss: 1.6671 - val_activation_loss: 0.8545 - val_activation_1_loss: 0.8126\nLoading model...\nPredicting OOF...\n172/172 [==============================] - 28s 142ms/step\nPredicting Test...\n111/111 [==============================] - 16s 142ms/step\n>>>> FOLD 5 Jaccard = 0.6999543571251045\n\n","output_type":"stream"}]},{"cell_type":"code","source":"print('>>>> OVERALL 5Fold CV Jaccard =',np.mean(jac))\n","metadata":{"execution":{"iopub.status.busy":"2023-11-26T04:59:38.482752Z","iopub.execute_input":"2023-11-26T04:59:38.483128Z","iopub.status.idle":"2023-11-26T04:59:38.488626Z","shell.execute_reply.started":"2023-11-26T04:59:38.483101Z","shell.execute_reply":"2023-11-26T04:59:38.487658Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":">>>> OVERALL 5Fold CV Jaccard = 0.7018045736467972\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}